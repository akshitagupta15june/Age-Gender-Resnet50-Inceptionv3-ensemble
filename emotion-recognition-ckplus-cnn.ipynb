{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":128470,"sourceType":"datasetVersion","datasetId":65125}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akshitagupta1506/emotion-recognition-ckplus-cnn?scriptVersionId=154571723\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd  # For data manipulation and analysis\nimport numpy as np  # For numerical operations and working with arrays\nimport cv2 as cv  # For image processing tasks\nimport os  # For interacting with the operating system, like file paths\nimport tensorflow as tf  # For building and training neural network models\n\nfrom keras.preprocessing.image import ImageDataGenerator  # For real-time data augmentation\nfrom tensorflow.keras.models import load_model  # For loading a saved Keras model\nfrom keras.models import Sequential  # For creating a linear stack of layers in the model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # For building model layers\nfrom keras.optimizers import Adam  # For optimization algorithms\nfrom keras.layers import BatchNormalization  # For applying Batch Normalization in neural network layers\nfrom keras.regularizers import l2  # For applying L2 regularization to prevent overfitting\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping  # Importing specific callback functions\n\nimport warnings  # For handling warnings\nimport sys  # For interacting with the Python interpreter\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")  # Ignore simple warnings if not already done\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore deprecation warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T17:56:30.770196Z","iopub.execute_input":"2023-12-11T17:56:30.770466Z","iopub.status.idle":"2023-12-11T17:56:42.937454Z","shell.execute_reply.started":"2023-12-11T17:56:30.770441Z","shell.execute_reply":"2023-12-11T17:56:42.936615Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\n\n# Define the path to your original dataset and the paths where you want to store your train and test datasets\noriginal_dataset_dir = '/kaggle/input/ckplus/CK+48'\ntrain_dir = 'CK+48_train'\ntest_dir = 'CK+48_test'\n\n# Create directories for training and testing datasets if they do not exist\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\n# Define the split ratio\ntrain_ratio = 0.8\n\n# Loop through each emotion category in the original dataset\nfor emotion in os.listdir(original_dataset_dir):\n    emotion_dir = os.path.join(original_dataset_dir, emotion)\n    if os.path.isdir(emotion_dir):\n        # Get a list of all the image filenames in the emotion category\n        images = [f for f in os.listdir(emotion_dir) if os.path.isfile(os.path.join(emotion_dir, f))]\n        \n        # Randomly shuffle the list of image filenames\n        np.random.shuffle(images)\n        \n        # Split the list of image filenames into training and testing sets\n        train_size = int(len(images) * train_ratio)\n        train_images = images[:train_size]\n        test_images = images[train_size:]\n        \n        # Create directories for the emotion category in the train and test datasets\n        train_emotion_dir = os.path.join(train_dir, emotion)\n        test_emotion_dir = os.path.join(test_dir, emotion)\n        os.makedirs(train_emotion_dir, exist_ok=True)\n        os.makedirs(test_emotion_dir, exist_ok=True)\n        \n        # Copy the images into the corresponding directories\n        for image in train_images:\n            shutil.copy(os.path.join(emotion_dir, image), os.path.join(train_emotion_dir, image))\n        for image in test_images:\n            shutil.copy(os.path.join(emotion_dir, image), os.path.join(test_emotion_dir, image))\n\nprint(\"Dataset splitting complete\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:56:42.939409Z","iopub.execute_input":"2023-12-11T17:56:42.939945Z","iopub.status.idle":"2023-12-11T17:56:48.378344Z","shell.execute_reply.started":"2023-12-11T17:56:42.939916Z","shell.execute_reply":"2023-12-11T17:56:48.377353Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Dataset splitting complete\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\n\n# Load the image\nimg = cv2.imread('/kaggle/input/smiling/42730583-she-got-beautiful-smile-portrait-of-cheerful-young-woman-looking-at-camera-and-smiling-while.jpg')\n\n# Get dimensions\nheight, width, channels = img.shape\nprint(f'Dimensions: {width}x{height}')\n# Create a data generator with augmentation\ntrain_data_generator = ImageDataGenerator(\n    rescale=1./255,  # Rescale the pixel values (normalization)\n    rotation_range=15,  # Random rotation in the range of 15 degrees\n    width_shift_range=0.15,  # Random horizontal shifts (15% of total width)\n    height_shift_range=0.15,  # Random vertical shifts (15% of total height)\n    shear_range=0.15,  # Random shearing transformations\n    zoom_range=0.15,  # Random zoom range\n    horizontal_flip=True,  # Randomly flip inputs horizontally\n)\n\n# Load images from the directory and apply the defined transformations\nfer_training_data = train_data_generator.flow_from_directory(\n    '/kaggle/working/CK+48_train',  # Path to the training data\n    target_size=(48, 48),  # Resize images to 48x48\n    batch_size=64,  # Number of images to yield per batch\n    color_mode='grayscale',  # Load images in grayscale\n    class_mode='categorical'  # Labels will be returned in categorical format\n)\n\nfer_training_data\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:13.69768Z","iopub.execute_input":"2023-12-11T17:59:13.698587Z","iopub.status.idle":"2023-12-11T17:59:13.773992Z","shell.execute_reply.started":"2023-12-11T17:59:13.698531Z","shell.execute_reply":"2023-12-11T17:59:13.773109Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Dimensions: 1300x867\nFound 783 images belonging to 7 classes.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<keras.src.preprocessing.image.DirectoryIterator at 0x7d791b58c910>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize an ImageDataGenerator for test data with rescaling\n# Rescales images by dividing pixel values by 255 (normalization)\ntest_data_generator = ImageDataGenerator(rescale=1./255)\n\n# Creates a data generator for the test dataset\n# flow_from_directory method loads images from a directory,\n# in this case, '/kaggle/working/CK+48_test'\nfer_test_data = test_data_generator.flow_from_directory(\n    '/kaggle/working/CK+48_test',  # Directory path for test images\n    target_size = (48, 48),  # Resizes images to 48x48 pixels\n    batch_size = 64,  # Number of images to yield per batch\n    color_mode = 'grayscale',  # Specifies that images are in grayscale\n    class_mode = 'categorical'  # Images are classified categorically\n)\n\n# fer_test_data is now a generator that yields batches of test images and their labels\nfer_test_data\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:18.895888Z","iopub.execute_input":"2023-12-11T17:59:18.896685Z","iopub.status.idle":"2023-12-11T17:59:18.918496Z","shell.execute_reply.started":"2023-12-11T17:59:18.896648Z","shell.execute_reply":"2023-12-11T17:59:18.917539Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 198 images belonging to 7 classes.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<keras.src.preprocessing.image.DirectoryIterator at 0x7d7925a68f70>"},"metadata":{}}]},{"cell_type":"code","source":"# Importing the optimizers module from TensorFlow's Keras library\nfrom tensorflow.keras import optimizers\n\n# Initializing a list of optimizers with specific configurations\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n    optimizers.Adam(0.001),\n]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:25.015817Z","iopub.execute_input":"2023-12-11T17:59:25.01648Z","iopub.status.idle":"2023-12-11T17:59:27.783057Z","shell.execute_reply.started":"2023-12-11T17:59:25.01645Z","shell.execute_reply":"2023-12-11T17:59:27.782284Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential()\n\nmodel.add(\n        Conv2D(\n            filters=512,\n            kernel_size=(5,5),\n            input_shape=(48, 48, 1),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_1'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_1'))\nmodel.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(5,5),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_2'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_2'))\n    \nmodel.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\nmodel.add(Dropout(0.25, name='dropout_1'))\n\nmodel.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_3'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_3'))\nmodel.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_4'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_4'))\n    \nmodel.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\nmodel.add(Dropout(0.25, name='dropout_2'))\n\nmodel.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_5'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_5'))\nmodel.add(\n        Conv2D(\n            filters=512,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_6'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_6'))\n    \nmodel.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\nmodel.add(Dropout(0.25, name='dropout_3'))\n\nmodel.add(Flatten(name='flatten'))\nmodel.add(\n        Dense(\n            256,\n            activation='elu',\n            kernel_initializer='he_normal',\n            name='dense_1'\n        )\n    )\nmodel.add(BatchNormalization(name='batchnorm_7'))\n    \nmodel.add(Dropout(0.25, name='dropout_4'))\n    \nmodel.add(\n        Dense(\n            7,\n            activation='softmax',\n            name='out_layer'\n        )\n    )\n    \nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy']\n    )\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:30.804136Z","iopub.execute_input":"2023-12-11T17:59:30.804752Z","iopub.status.idle":"2023-12-11T17:59:31.341856Z","shell.execute_reply.started":"2023-12-11T17:59:30.804716Z","shell.execute_reply":"2023-12-11T17:59:31.340982Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 48, 48, 512)       13312     \n                                                                 \n batchnorm_1 (BatchNormaliz  (None, 48, 48, 512)       2048      \n ation)                                                          \n                                                                 \n conv2d_2 (Conv2D)           (None, 48, 48, 256)       3277056   \n                                                                 \n batchnorm_2 (BatchNormaliz  (None, 48, 48, 256)       1024      \n ation)                                                          \n                                                                 \n maxpool2d_1 (MaxPooling2D)  (None, 24, 24, 256)       0         \n                                                                 \n dropout_1 (Dropout)         (None, 24, 24, 256)       0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 24, 24, 128)       295040    \n                                                                 \n batchnorm_3 (BatchNormaliz  (None, 24, 24, 128)       512       \n ation)                                                          \n                                                                 \n conv2d_4 (Conv2D)           (None, 24, 24, 128)       147584    \n                                                                 \n batchnorm_4 (BatchNormaliz  (None, 24, 24, 128)       512       \n ation)                                                          \n                                                                 \n maxpool2d_2 (MaxPooling2D)  (None, 12, 12, 128)       0         \n                                                                 \n dropout_2 (Dropout)         (None, 12, 12, 128)       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 12, 12, 256)       295168    \n                                                                 \n batchnorm_5 (BatchNormaliz  (None, 12, 12, 256)       1024      \n ation)                                                          \n                                                                 \n conv2d_6 (Conv2D)           (None, 12, 12, 512)       1180160   \n                                                                 \n batchnorm_6 (BatchNormaliz  (None, 12, 12, 512)       2048      \n ation)                                                          \n                                                                 \n maxpool2d_3 (MaxPooling2D)  (None, 6, 6, 512)         0         \n                                                                 \n dropout_3 (Dropout)         (None, 6, 6, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 18432)             0         \n                                                                 \n dense_1 (Dense)             (None, 256)               4718848   \n                                                                 \n batchnorm_7 (BatchNormaliz  (None, 256)               1024      \n ation)                                                          \n                                                                 \n dropout_4 (Dropout)         (None, 256)               0         \n                                                                 \n out_layer (Dense)           (None, 7)                 1799      \n                                                                 \n=================================================================\nTotal params: 9937159 (37.91 MB)\nTrainable params: 9933063 (37.89 MB)\nNon-trainable params: 4096 (16.00 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reduce learning rate when a metric has stopped improving\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00005,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    factor=0.5,\n    patience=7,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:37.611771Z","iopub.execute_input":"2023-12-11T17:59:37.612462Z","iopub.status.idle":"2023-12-11T17:59:37.618159Z","shell.execute_reply.started":"2023-12-11T17:59:37.612426Z","shell.execute_reply":"2023-12-11T17:59:37.617247Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"main_folder = \"../input/celeba-dataset/\"\nimages_folder = main_folder + \"img_align_celeba/img_align_celeba/\"\n\nexample_pic = images_folder + \"000506.jpg\"\n\ntraining_sample = 10000\nvalidation_sample = 2000\ntest_sample = 2000\nimg_width = 178\nimg_height = 218\nbatch_size = 16\nnum_epochs = 5","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:40.195991Z","iopub.execute_input":"2023-12-11T17:59:40.196373Z","iopub.status.idle":"2023-12-11T17:59:40.201893Z","shell.execute_reply.started":"2023-12-11T17:59:40.196343Z","shell.execute_reply":"2023-12-11T17:59:40.200606Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nhistory = model.fit(\n    fer_training_data,\n    epochs=60, \n    validation_data=fer_test_data,\n    batch_size = 64,\n    callbacks=callbacks,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T17:59:42.332858Z","iopub.execute_input":"2023-12-11T17:59:42.333772Z","iopub.status.idle":"2023-12-11T18:01:45.061146Z","shell.execute_reply.started":"2023-12-11T17:59:42.333731Z","shell.execute_reply":"2023-12-11T18:01:45.060142Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/60\n","output_type":"stream"},{"name":"stderr","text":"2023-12-11 17:59:44.480739: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"13/13 [==============================] - 21s 320ms/step - loss: 2.4218 - accuracy: 0.3231 - val_loss: 119.6251 - val_accuracy: 0.2172 - lr: 0.0010\nEpoch 2/60\n13/13 [==============================] - 2s 129ms/step - loss: 1.5477 - accuracy: 0.4585 - val_loss: 48.2479 - val_accuracy: 0.2525 - lr: 0.0010\nEpoch 3/60\n13/13 [==============================] - 2s 127ms/step - loss: 1.3320 - accuracy: 0.5172 - val_loss: 23.4721 - val_accuracy: 0.2273 - lr: 0.0010\nEpoch 4/60\n13/13 [==============================] - 2s 130ms/step - loss: 1.1446 - accuracy: 0.5849 - val_loss: 7.5799 - val_accuracy: 0.2778 - lr: 0.0010\nEpoch 5/60\n13/13 [==============================] - 2s 130ms/step - loss: 1.0449 - accuracy: 0.6271 - val_loss: 5.9323 - val_accuracy: 0.2828 - lr: 0.0010\nEpoch 6/60\n13/13 [==============================] - 2s 130ms/step - loss: 0.9985 - accuracy: 0.6424 - val_loss: 5.5341 - val_accuracy: 0.3535 - lr: 0.0010\nEpoch 7/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.8740 - accuracy: 0.6960 - val_loss: 3.5544 - val_accuracy: 0.3889 - lr: 0.0010\nEpoch 8/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.7917 - accuracy: 0.6922 - val_loss: 2.3523 - val_accuracy: 0.5000 - lr: 0.0010\nEpoch 9/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.7820 - accuracy: 0.7139 - val_loss: 1.1733 - val_accuracy: 0.6818 - lr: 0.0010\nEpoch 10/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.6824 - accuracy: 0.7484 - val_loss: 1.6925 - val_accuracy: 0.5556 - lr: 0.0010\nEpoch 11/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.6468 - accuracy: 0.7829 - val_loss: 1.0557 - val_accuracy: 0.7121 - lr: 0.0010\nEpoch 12/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.6376 - accuracy: 0.7637 - val_loss: 1.4519 - val_accuracy: 0.6263 - lr: 0.0010\nEpoch 13/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.5638 - accuracy: 0.8072 - val_loss: 0.8017 - val_accuracy: 0.7525 - lr: 0.0010\nEpoch 14/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.5295 - accuracy: 0.8059 - val_loss: 0.9393 - val_accuracy: 0.7222 - lr: 0.0010\nEpoch 15/60\n13/13 [==============================] - 2s 130ms/step - loss: 0.5754 - accuracy: 0.7995 - val_loss: 0.4589 - val_accuracy: 0.8182 - lr: 0.0010\nEpoch 16/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.5598 - accuracy: 0.7905 - val_loss: 0.8730 - val_accuracy: 0.7273 - lr: 0.0010\nEpoch 17/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.4793 - accuracy: 0.8161 - val_loss: 0.4886 - val_accuracy: 0.8081 - lr: 0.0010\nEpoch 18/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.4813 - accuracy: 0.8263 - val_loss: 0.4612 - val_accuracy: 0.8182 - lr: 0.0010\nEpoch 19/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.4756 - accuracy: 0.8161 - val_loss: 0.3893 - val_accuracy: 0.8535 - lr: 0.0010\nEpoch 20/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.3949 - accuracy: 0.8825 - val_loss: 0.6773 - val_accuracy: 0.7879 - lr: 0.0010\nEpoch 21/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.3593 - accuracy: 0.8787 - val_loss: 0.3890 - val_accuracy: 0.8788 - lr: 0.0010\nEpoch 22/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.3734 - accuracy: 0.8710 - val_loss: 0.3206 - val_accuracy: 0.8838 - lr: 0.0010\nEpoch 23/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.4013 - accuracy: 0.8557 - val_loss: 0.4098 - val_accuracy: 0.8434 - lr: 0.0010\nEpoch 24/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.4565 - accuracy: 0.8416 - val_loss: 0.5563 - val_accuracy: 0.8384 - lr: 0.0010\nEpoch 25/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.4035 - accuracy: 0.8455 - val_loss: 0.2882 - val_accuracy: 0.9040 - lr: 0.0010\nEpoch 26/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.4079 - accuracy: 0.8404 - val_loss: 0.5842 - val_accuracy: 0.7778 - lr: 0.0010\nEpoch 27/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.3357 - accuracy: 0.8799 - val_loss: 0.2992 - val_accuracy: 0.8889 - lr: 0.0010\nEpoch 28/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.3495 - accuracy: 0.8787 - val_loss: 0.3678 - val_accuracy: 0.8687 - lr: 0.0010\nEpoch 29/60\n13/13 [==============================] - 2s 131ms/step - loss: 0.3034 - accuracy: 0.8876 - val_loss: 0.2314 - val_accuracy: 0.9242 - lr: 0.0010\nEpoch 30/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2969 - accuracy: 0.8991 - val_loss: 0.3292 - val_accuracy: 0.8838 - lr: 0.0010\nEpoch 31/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.2787 - accuracy: 0.9132 - val_loss: 0.2165 - val_accuracy: 0.9343 - lr: 0.0010\nEpoch 32/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2390 - accuracy: 0.9144 - val_loss: 0.4669 - val_accuracy: 0.8384 - lr: 0.0010\nEpoch 33/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2473 - accuracy: 0.9017 - val_loss: 0.3376 - val_accuracy: 0.8535 - lr: 0.0010\nEpoch 34/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2270 - accuracy: 0.9195 - val_loss: 0.3894 - val_accuracy: 0.8232 - lr: 0.0010\nEpoch 35/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2360 - accuracy: 0.9068 - val_loss: 0.2002 - val_accuracy: 0.9192 - lr: 0.0010\nEpoch 36/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1989 - accuracy: 0.9234 - val_loss: 0.2241 - val_accuracy: 0.9242 - lr: 0.0010\nEpoch 37/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2398 - accuracy: 0.9221 - val_loss: 0.1769 - val_accuracy: 0.9293 - lr: 0.0010\nEpoch 38/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.2653 - accuracy: 0.9055 - val_loss: 0.1317 - val_accuracy: 0.9596 - lr: 0.0010\nEpoch 39/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.2951 - accuracy: 0.8812 - val_loss: 0.1571 - val_accuracy: 0.9343 - lr: 0.0010\nEpoch 40/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2598 - accuracy: 0.9080 - val_loss: 0.3844 - val_accuracy: 0.8687 - lr: 0.0010\nEpoch 41/60\n13/13 [==============================] - 2s 135ms/step - loss: 0.2333 - accuracy: 0.9195 - val_loss: 0.1317 - val_accuracy: 0.9495 - lr: 0.0010\nEpoch 42/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.2389 - accuracy: 0.9093 - val_loss: 0.2032 - val_accuracy: 0.8939 - lr: 0.0010\nEpoch 43/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1874 - accuracy: 0.9387 - val_loss: 0.3032 - val_accuracy: 0.8687 - lr: 0.0010\nEpoch 44/60\n13/13 [==============================] - 2s 130ms/step - loss: 0.1912 - accuracy: 0.9387 - val_loss: 0.0937 - val_accuracy: 0.9798 - lr: 0.0010\nEpoch 45/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.1634 - accuracy: 0.9438 - val_loss: 0.1459 - val_accuracy: 0.9394 - lr: 0.0010\nEpoch 46/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1543 - accuracy: 0.9527 - val_loss: 0.1148 - val_accuracy: 0.9495 - lr: 0.0010\nEpoch 47/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1577 - accuracy: 0.9476 - val_loss: 0.1381 - val_accuracy: 0.9444 - lr: 0.0010\nEpoch 48/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.1458 - accuracy: 0.9527 - val_loss: 0.1766 - val_accuracy: 0.9242 - lr: 0.0010\nEpoch 49/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.2269 - accuracy: 0.9336 - val_loss: 0.1208 - val_accuracy: 0.9495 - lr: 0.0010\nEpoch 50/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1726 - accuracy: 0.9374 - val_loss: 0.1275 - val_accuracy: 0.9444 - lr: 0.0010\nEpoch 51/60\n13/13 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9553\nEpoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n13/13 [==============================] - 2s 127ms/step - loss: 0.1364 - accuracy: 0.9553 - val_loss: 0.2776 - val_accuracy: 0.8990 - lr: 0.0010\nEpoch 52/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1720 - accuracy: 0.9425 - val_loss: 0.0578 - val_accuracy: 0.9798 - lr: 5.0000e-04\nEpoch 53/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.1091 - accuracy: 0.9719 - val_loss: 0.0447 - val_accuracy: 0.9899 - lr: 5.0000e-04\nEpoch 54/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.1059 - accuracy: 0.9617 - val_loss: 0.0306 - val_accuracy: 0.9949 - lr: 5.0000e-04\nEpoch 55/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1068 - accuracy: 0.9668 - val_loss: 0.0359 - val_accuracy: 0.9899 - lr: 5.0000e-04\nEpoch 56/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.1129 - accuracy: 0.9642 - val_loss: 0.0432 - val_accuracy: 0.9899 - lr: 5.0000e-04\nEpoch 57/60\n13/13 [==============================] - 2s 128ms/step - loss: 0.1190 - accuracy: 0.9604 - val_loss: 0.0311 - val_accuracy: 0.9848 - lr: 5.0000e-04\nEpoch 58/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.0853 - accuracy: 0.9732 - val_loss: 0.0405 - val_accuracy: 0.9899 - lr: 5.0000e-04\nEpoch 59/60\n13/13 [==============================] - 2s 127ms/step - loss: 0.0787 - accuracy: 0.9796 - val_loss: 0.0392 - val_accuracy: 0.9949 - lr: 5.0000e-04\nEpoch 60/60\n13/13 [==============================] - 2s 129ms/step - loss: 0.1023 - accuracy: 0.9693 - val_loss: 0.0685 - val_accuracy: 0.9848 - lr: 5.0000e-04\n","output_type":"stream"}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\n\n# Training vs. Validation Accuracy\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['accuracy']) + 1)), y=history.history['accuracy'], mode='lines+markers', name='Training Accuracy'))\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_accuracy']) + 1)), y=history.history['val_accuracy'], mode='lines+markers', name='Validation Accuracy'))\n\n# Layout for Accuracy\nfig.update_layout(title='Training vs. Validation Accuracy', xaxis_title='Epoch', yaxis_title='Accuracy', template=\"plotly_white\")\n\n# Show the plot\nfig.show()\n\n# New figure for loss\nfig = go.Figure()\n\n# Training vs. Validation Loss\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss']) + 1)), y=history.history['loss'], mode='lines+markers', name='Training Loss'))\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss']) + 1)), y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'))\n\n# Layout for Loss\nfig.update_layout(title='Training vs. Validation Loss', xaxis_title='Epoch', yaxis_title='Loss', template=\"plotly_white\")\n\n# Show the plot\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:01:45.063189Z","iopub.execute_input":"2023-12-11T18:01:45.063529Z","iopub.status.idle":"2023-12-11T18:01:46.542289Z","shell.execute_reply.started":"2023-12-11T18:01:45.063498Z","shell.execute_reply":"2023-12-11T18:01:46.541373Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.25.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"e851e36d-366d-45d5-a92d-fabefd57e1c6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e851e36d-366d-45d5-a92d-fabefd57e1c6\")) {                    Plotly.newPlot(                        \"e851e36d-366d-45d5-a92d-fabefd57e1c6\",                        [{\"mode\":\"lines+markers\",\"name\":\"Training Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60],\"y\":[0.3231162130832672,0.4584929645061493,0.517241358757019,0.5849297642707825,0.6270753741264343,0.6424010396003723,0.6960408687591553,0.6922094225883484,0.7139208316802979,0.7484035491943359,0.7828863263130188,0.7637292742729187,0.8071519732475281,0.8058748245239258,0.7994891405105591,0.7905491590499878,0.8160919547080994,0.826309084892273,0.8160919547080994,0.8825032114982605,0.8786717653274536,0.8710089325904846,0.8556832671165466,0.8416347503662109,0.845466136932373,0.8403576016426086,0.8799489140510559,0.8786717653274536,0.8876117467880249,0.8991060256958008,0.9131545424461365,0.9144316911697388,0.9016602635383606,0.9195402264595032,0.9067688584327698,0.9233716726303101,0.9220945239067078,0.9054917097091675,0.8812260627746582,0.9080459475517273,0.9195402264595032,0.9093230962753296,0.938697338104248,0.938697338104248,0.9438058733940125,0.9527458548545837,0.9476373195648193,0.9527458548545837,0.9335887432098389,0.9374201893806458,0.9553001523017883,0.9425287246704102,0.9719029664993286,0.961685836315155,0.9667943716049194,0.9642400741577148,0.9604086875915527,0.9731800556182861,0.9795657992362976,0.969348669052124],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Validation Accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60],\"y\":[0.21717171370983124,0.2525252401828766,0.22727273404598236,0.2777777910232544,0.28282827138900757,0.35353535413742065,0.3888888955116272,0.5,0.6818181872367859,0.5555555820465088,0.7121211886405945,0.6262626051902771,0.752525269985199,0.7222222089767456,0.8181818127632141,0.7272727489471436,0.808080792427063,0.8181818127632141,0.8535353541374207,0.7878788113594055,0.8787878751754761,0.8838383555412292,0.8434343338012695,0.8383838534355164,0.9040403962135315,0.7777777910232544,0.8888888955116272,0.868686854839325,0.9242424368858337,0.8838383555412292,0.9343434572219849,0.8383838534355164,0.8535353541374207,0.8232323527336121,0.9191918969154358,0.9242424368858337,0.9292929172515869,0.9595959782600403,0.9343434572219849,0.868686854839325,0.9494949579238892,0.8939393758773804,0.868686854839325,0.9797979593276978,0.939393937587738,0.9494949579238892,0.9444444179534912,0.9242424368858337,0.9494949579238892,0.9444444179534912,0.8989899158477783,0.9797979593276978,0.9898989796638489,0.9949495196342468,0.9898989796638489,0.9898989796638489,0.9848484992980957,0.9898989796638489,0.9949495196342468,0.9848484992980957],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Training vs. Validation Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('e851e36d-366d-45d5-a92d-fabefd57e1c6');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"b3821d63-5df2-49ec-b71b-44872c3a14a6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b3821d63-5df2-49ec-b71b-44872c3a14a6\")) {                    Plotly.newPlot(                        \"b3821d63-5df2-49ec-b71b-44872c3a14a6\",                        [{\"mode\":\"lines+markers\",\"name\":\"Training Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60],\"y\":[2.4218130111694336,1.547654628753662,1.3320292234420776,1.144615650177002,1.0449366569519043,0.9984902739524841,0.8740234375,0.791713297367096,0.7820285558700562,0.6823527216911316,0.6468479633331299,0.6376029253005981,0.5638048052787781,0.5294715762138367,0.5754255056381226,0.5597972273826599,0.4792817234992981,0.48133739829063416,0.47556665539741516,0.3949396312236786,0.3592909574508667,0.37338927388191223,0.40131649374961853,0.4565081298351288,0.4034746289253235,0.40789327025413513,0.3357374668121338,0.3495444059371948,0.303435355424881,0.29685723781585693,0.27871546149253845,0.23899102210998535,0.24731868505477905,0.22697658836841583,0.23603075742721558,0.1988937109708786,0.23976677656173706,0.2653444707393646,0.2950858473777771,0.25977084040641785,0.23328284919261932,0.23892559111118317,0.18735100328922272,0.19117645919322968,0.1634058952331543,0.15433505177497864,0.1576908528804779,0.14580638706684113,0.22694812715053558,0.172641783952713,0.13638287782669067,0.17199821770191193,0.10910563915967941,0.10588271170854568,0.10680016130208969,0.1128799095749855,0.11897552758455276,0.08527481555938721,0.07866514474153519,0.10228454321622849],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Validation Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60],\"y\":[119.62506866455078,48.247894287109375,23.472074508666992,7.579884052276611,5.932252407073975,5.534110069274902,3.554439067840576,2.3522725105285645,1.1733089685440063,1.6924949884414673,1.0557233095169067,1.451922059059143,0.8017356395721436,0.9393033385276794,0.45888185501098633,0.8729870915412903,0.48861321806907654,0.4611743688583374,0.38929980993270874,0.6772926449775696,0.3890099823474884,0.32063546776771545,0.40975597500801086,0.5562666654586792,0.28816381096839905,0.5842145681381226,0.29915371537208557,0.367768257856369,0.2313620001077652,0.329191654920578,0.21646015346050262,0.46694415807724,0.337600439786911,0.38936495780944824,0.20015576481819153,0.2241058498620987,0.17690841853618622,0.13172578811645508,0.157136470079422,0.384426087141037,0.13174360990524292,0.20316584408283234,0.3032381534576416,0.0936567634344101,0.14586973190307617,0.1148405522108078,0.13809971511363983,0.17662975192070007,0.12082783877849579,0.12754392623901367,0.2775766849517822,0.057785965502262115,0.04471752792596817,0.03063971735537052,0.03594275563955307,0.04324840381741524,0.031060323119163513,0.04051397740840912,0.03923150897026062,0.06845120340585709],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Training vs. Validation Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('b3821d63-5df2-49ec-b71b-44872c3a14a6');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the validation dataset\nloss, accuracy = model.evaluate(fer_test_data, verbose=1)\n\n# Print the loss and accuracy\nprint(f'Validation Loss: {loss}')\nprint(f'Validation Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:01:46.543508Z","iopub.execute_input":"2023-12-11T18:01:46.543893Z","iopub.status.idle":"2023-12-11T18:01:46.78215Z","shell.execute_reply.started":"2023-12-11T18:01:46.54386Z","shell.execute_reply":"2023-12-11T18:01:46.781198Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 0s 28ms/step - loss: 0.0685 - accuracy: 0.9848\nValidation Loss: 0.06845103204250336\nValidation Accuracy: 98.48%\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Load the image (replace 'path_to_your_image' with the actual path)\nimage = cv2.imread('/kaggle/input/smiling/42730583-she-got-beautiful-smile-portrait-of-cheerful-young-woman-looking-at-camera-and-smiling-while.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert color format to RGB\n\n# Preprocess the image (adjust as per your model's requirements)\n# For example, resize the image to match the input size of your model\ndesired_width, desired_height = 48, 48  # Adjust as needed\nimage = cv2.resize(image, (desired_width, desired_height))\nimage = image / 255.0  # Normalize the pixel values to the range [0, 1]\n\n# Ensure the input shape matches the model's expectation for RGB images\nimage = np.reshape(image, (1, desired_width, desired_height, 3))\n\n# Make predictions using your trained model\npredictions = model.predict(image)\n\n# You can then use the predictions as needed, depending on your use case\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:09:52.553355Z","iopub.execute_input":"2023-12-11T18:09:52.554174Z","iopub.status.idle":"2023-12-11T18:09:52.686684Z","shell.execute_reply.started":"2023-12-11T18:09:52.554143Z","shell.execute_reply":"2023-12-11T18:09:52.685363Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(image, (\u001b[38;5;241m1\u001b[39m, desired_width, desired_height, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions using your trained model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# You can then use the predictions as needed, depending on your use case\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filespao406t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv2d_1\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 48, 48, 3)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n       inputs=tf.Tensor(shape=(None, 48, 48, 3), dtype=float32)\n       training=False\n       mask=None\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv2d_1\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 48, 48, 3)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n       inputs=tf.Tensor(shape=(None, 48, 48, 3), dtype=float32)\n       training=False\n       mask=None\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}